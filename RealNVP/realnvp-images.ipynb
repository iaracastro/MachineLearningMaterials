{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "from pathlib import Path\n",
    "from types import FrameType\n",
    "from typing import Generator, Iterable, Literal, Self, TypeAlias\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from torch import nn\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "sns.set_theme()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "Shape2d: TypeAlias = tuple[int, int, int]\n",
    "MaskMap: TypeAlias = dict[Shape2d, tuple[torch.Tensor, torch.Tensor]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_finite(t: torch.Tensor) -> np.ndarray:\n",
    "    return np.isfinite(t.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "def check_parameters(model: nn.Module) -> bool:\n",
    "    def check_nested(l: nn.Module | torch.Tensor, name: str) -> bool:\n",
    "        trouble = False\n",
    "        for i, attr in enumerate(l):\n",
    "            if isinstance(attr, nn.Module):\n",
    "                next_name = f\"{name} -> {i}:{attr.__class__.__name__}\"\n",
    "                if isinstance(attr, Iterable):\n",
    "                    trouble = check_nested(attr, next_name) or trouble\n",
    "                else:\n",
    "                    trouble = check_nested(attr.parameters(), next_name) or trouble\n",
    "            elif not is_finite(attr).all():\n",
    "                print(f\"Trouble at {name} -> {i}:{attr.__class__.__name__}\")\n",
    "                trouble = True\n",
    "        return trouble\n",
    "\n",
    "    return any(check_nested(module, name) for name, module in model.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://github.com/bjlkeng/sandbox/blob/master/realnvp/pytorch-realnvp-cifar10.ipynb\n",
    "partition_masks: MaskMap = {}\n",
    "channel_masks: MaskMap = {}\n",
    "\n",
    "\n",
    "def partition_mask(shape: Shape2d):\n",
    "    \"\"\"\n",
    "    Partitions the input image into two sets of variables with a binary mask with a checkerboard\n",
    "    pattern. The pattern alternates between 0 and 1 for each pixel in the image.\n",
    "    \"\"\"\n",
    "    global partition_masks, device\n",
    "    if shape not in partition_masks:\n",
    "        mask = torch.tensor(\n",
    "            1 - np.indices(shape).sum(axis=0) % 2, device=device\n",
    "        )\n",
    "        partition_masks[shape] = (mask, 1 - mask)\n",
    "    return partition_masks[shape]\n",
    "\n",
    "\n",
    "def channel_mask(shape: Shape2d):\n",
    "    \"\"\"\n",
    "    Segregrates the channels into two groups, using a mask that is 0 for the first half of the\n",
    "    channels and 1 for the second half, so that transformations are independently applied.\n",
    "    \"\"\"\n",
    "    global channel_masks, device\n",
    "    if shape not in channel_masks:\n",
    "        mask = torch.cat(\n",
    "            [\n",
    "                torch.zeros((shape[0] // 2, shape[1], shape[2]), device=device),\n",
    "                torch.ones((shape[0] // 2, shape[1], shape[2]), device=device),\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "        channel_masks[shape] = (mask, 1 - mask)\n",
    "    return channel_masks[shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "epsilon = torch.finfo(torch.get_default_dtype()).eps\n",
    "\n",
    "\n",
    "# def preprocessing(x: torch.Tensor, noise_factor: float = 0.01) -> torch.Tensor:\n",
    "#     global epsilon\n",
    "#     noise = torch.rand_like(x)\n",
    "#     noise -= 0.5\n",
    "#     noise *= 2 * noise_factor\n",
    "#     x += noise\n",
    "#     return torch.logit(x, epsilon, out=x)\n",
    "\n",
    "\n",
    "def preprocessing(x: torch.Tensor) -> torch.Tensor:\n",
    "    global epsilon\n",
    "    # [0, 1] -> [0, 255]\n",
    "    x *= 255.\n",
    "    # Adiciona ruído uniforme [0, 1] aleatório\n",
    "    x += torch.rand_like(x)\n",
    "    x /= 255\n",
    "\n",
    "    # Evita valores próximos dos extremos\n",
    "    #x *= 1 - 2 * alpha\n",
    "    #x += alpha\n",
    "    #x = torch.logit(x, epsilon, out=x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def inverse_processing(y: torch.Tensor) -> torch.Tensor:\n",
    "    #y = torch.sigmoid(y)\n",
    "    #y -= alpha\n",
    "    #y /= 1 - 2 * alpha\n",
    "    #y *= 256\n",
    "    #y = torch.clip(y, min=0, max=255, out=y)\n",
    "    #y /= 255\n",
    "    y = torch.clip(y, min=0, max=1)\n",
    "    return y\n",
    "\n",
    "\n",
    "def log_preprocessing_grad(y: torch.Tensor, alpha: float = 0.05):\n",
    "    # Used to adjust for pixel preprocessing\n",
    "    # Assume input is y = preprocessing(x)\n",
    "    comp = 1 - 2 * alpha\n",
    "    x = 256 / comp * (torch.sigmoid(y) - alpha)\n",
    "    arg = comp * x / 256 + alpha\n",
    "    return torch.log(1 / arg + 1 / (1 - arg)) + np.log(comp / 256)\n",
    "\n",
    "\n",
    "def to_device(x: torch.Tensor) -> torch.Tensor:\n",
    "    global device\n",
    "    return x.to(device)\n",
    "\n",
    "\n",
    "preprocessing_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(preprocessing),\n",
    "    transforms.Lambda(to_device)\n",
    "])\n",
    "\n",
    "kwargs = dict(\n",
    "    download=True, transform=preprocessing_transform\n",
    ")\n",
    "\n",
    "# TODO FIX ME\n",
    "# debug_data = []\n",
    "# for i, x in enumerate(train_dataset):\n",
    "#     if i >= 200:\n",
    "#         break\n",
    "#     debug_data.append(x)\n",
    "\n",
    "train_dataset_cifar = datasets.MNIST(\"data\", train=True, **kwargs)\n",
    "test_dataset_cifar = datasets.MNIST(\"data\", train=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for Normalizing Flows with Real NVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.batchnorm import _NormBase\n",
    "\n",
    "\n",
    "class PaperBatchNorm2d(_NormBase):\n",
    "    \"\"\"\n",
    "    Partially based on:\n",
    "        https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d\n",
    "        https://discuss.pytorch.org/t/implementing-batchnorm-in-pytorch-problem-with-updating-self-running-mean-and-self-running-var/49314/5\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        eps: float = 1e-5,\n",
    "        momentum: float = 0.005,\n",
    "        device: str | torch.device | None = None,\n",
    "        dtype: torch.dtype | None = None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            num_features, eps, momentum, affine=False, track_running_stats=True, device=device, dtype=dtype\n",
    "        )\n",
    "\n",
    "    def _check_input_dim(self, input: torch.Tensor) -> None:\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError(f\"Expected 4D input (got {input.dim()}D input)\")\n",
    "\n",
    "    def forward(self, input: torch.Tensor, testing: bool = False) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        self._check_input_dim(input)\n",
    "\n",
    "        if self.training:\n",
    "            # Note: Need to detach `running_{mean,var}` so we don't backwards propagate through them\n",
    "            unbiased_var, tmean = torch.var_mean(input, [0, 2, 3], unbiased=True)\n",
    "            mean = torch.mean(input, [0, 2, 3]) # along channel axis\n",
    "            unbiased_var = torch.var(input, [0, 2, 3], unbiased=True) # along channel axis\n",
    "            running_mean = (1.0 - self.momentum) * self.running_mean.detach() + self.momentum * mean\n",
    "\n",
    "            # Strange: PyTorch impl. of running variance uses biased_variance for the batch calc but\n",
    "            # *unbiased_var* for the running_var!\n",
    "            # https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Normalization.cpp#L190\n",
    "            running_var = (1.0 - self.momentum) * self.running_var.detach() + self.momentum * unbiased_var\n",
    "\n",
    "            # BK: Modification from the paper to use running mean/var instead of batch mean/var\n",
    "            # change shape\n",
    "            current_mean = running_mean.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "            current_var = running_var.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "\n",
    "            denom = (current_var + self.eps)\n",
    "            y = (input - current_mean) / denom.sqrt()\n",
    "\n",
    "            self.running_mean = running_mean\n",
    "            self.running_var = running_var\n",
    "        else:\n",
    "            current_mean = self.running_mean.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "            current_var = self.running_var.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "            denom = (current_var + self.eps)\n",
    "\n",
    "            if testing:\n",
    "                # Reverse operation for testing\n",
    "                y = input * denom.sqrt() + current_mean\n",
    "            else:\n",
    "                y = (input - current_mean) / denom.sqrt()\n",
    "\n",
    "        return y, -0.5 * torch.log(denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output: TypeAlias = tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    conv_kwargs = dict(kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=False)\n",
    "    conv_kwargs2 = dict(kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "    def __init__(\n",
    "        self, in_planes: int, out_planes: int, norm_type: type[_NormBase] = nn.BatchNorm2d\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            weight_norm(nn.Conv2d(in_planes, out_planes, **self.conv_kwargs)),\n",
    "            norm_type(out_planes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            weight_norm(nn.Conv2d(out_planes, out_planes, **self.conv_kwargs)),\n",
    "            norm_type(out_planes)\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv_block(x)\n",
    "        out += x\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class NormalizingFlowNVP(nn.Module):\n",
    "    TWO_PI = torch.tensor(2 * torch.pi, device=device)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_coupling: int = 18,\n",
    "        num_final_coupling: int = 4,\n",
    "        planes: int = 64,\n",
    "        norm_type: Literal[\"batch\", \"instance\"] = \"batch\",\n",
    "        shape: Shape2d = (3, 32, 32),\n",
    "        learning_rate: float = 1e-6,\n",
    "        l2_regularization: float = 5e-5,\n",
    "        weight_decay: float | None = None,\n",
    "        gradient_clip_norm: float | None = None,\n",
    "        seed: int | None = None,\n",
    "        save_path: Path | str | None = \"models/best_model.pth\",\n",
    "        step_size: int = 5,\n",
    "        gamma: float = 0.2\n",
    "    ) -> None:\n",
    "        global device\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        super().__init__()\n",
    "        # Number of initial coupling layers\n",
    "        self.num_coupling = num_coupling\n",
    "        # Number of final coupling layers\n",
    "        self.num_final_coupling = num_final_coupling\n",
    "        # Shape of the input image\n",
    "        self.shape = shape\n",
    "        self.norm_layer = nn.BatchNorm2d if norm_type == \"batch\" else nn.InstanceNorm2d\n",
    "\n",
    "        # Number of output planes in the convolutional layers\n",
    "        self.planes = planes\n",
    "        # Scaling functions for each coupling layer\n",
    "        self.s = nn.ModuleList()\n",
    "        # Translation functions for each coupling layer\n",
    "        self.t = nn.ModuleList()\n",
    "        # List of batch normalization layers\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        # Learnable scalar scaling parameters for outputs of s and t\n",
    "        self.s_scale = nn.ParameterList()\n",
    "        self.t_scale = nn.ParameterList()\n",
    "        self.t_bias = nn.ParameterList()\n",
    "        self.shapes: list[Shape2d] = []\n",
    "        # Change shape and planes to increase model's capacity\n",
    "        for i in range(num_coupling):\n",
    "            self._append_transformations(shape, planes)\n",
    "            if i % 6 == 2:\n",
    "                shape = (4 * shape[0], shape[1] // 2, shape[2] // 2)\n",
    "            if i % 6 == 5:\n",
    "                # Factoring out half the channels\n",
    "                shape = (shape[0] // 2, shape[1], shape[2])\n",
    "                planes = 2 * planes\n",
    "\n",
    "        # Setup final coupling layers with possibly different configurations\n",
    "        for i in range(num_final_coupling):\n",
    "            self._append_transformations(shape, planes)\n",
    "\n",
    "        self.epoch = 1\n",
    "        self.to(device)\n",
    "        self.should_stop = False\n",
    "        self.l2_regularization = l2_regularization\n",
    "        self.gradient_clip_norm = gradient_clip_norm\n",
    "        self.train_loss_history: list[float] = []\n",
    "        self.validation_loss_history: list[float] = []\n",
    "        self.save_path = None if save_path is None else Path(save_path)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(), learning_rate, weight_decay=weight_decay or 0.0\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size= step_size, gamma=gamma\n",
    "        )\n",
    "\n",
    "    def _conv_stack(self, input_planes: int, internal_planes: int) -> nn.Sequential:\n",
    "        # A common stack of convolutional blocks used in s and t functions\n",
    "        return nn.Sequential(\n",
    "            weight_norm(nn.Conv2d(input_planes, internal_planes, **ConvBlock.conv_kwargs)),\n",
    "            #self.norm_layer(internal_planes),\n",
    "            #ConvBlock(internal_planes, internal_planes),\n",
    "            #ConvBlock(internal_planes, internal_planes),\n",
    "            ConvBlock(internal_planes, internal_planes),\n",
    "            ConvBlock(internal_planes, internal_planes),\n",
    "            weight_norm(nn.Conv2d(internal_planes, input_planes, **ConvBlock.conv_kwargs)),\n",
    "            #self.norm_layer(input_planes)\n",
    "        )\n",
    "\n",
    "    def _append_transformations(self, shape: Shape2d, planes: int) -> None:\n",
    "        input_planes = shape[0]\n",
    "        # Append scaling and translation functions for each coupling layer\n",
    "        self.s.append(self._conv_stack(input_planes, planes))\n",
    "        self.t.append(self._conv_stack(input_planes, planes))\n",
    "        for parameter_list in (self.s_scale, self.t_scale, self.t_bias):\n",
    "            parameter_list.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "        self.norms.append(PaperBatchNorm2d(input_planes))\n",
    "        self.shapes.append(shape)\n",
    "\n",
    "    def _get_binary_masks(self, shape: int, layer_index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Apply mask to manage which parts of the data are transformed\n",
    "        if layer_index < self.num_coupling:\n",
    "            binary_masks = partition_mask(shape) if layer_index % 6 < 3 else channel_mask(shape)\n",
    "        elif layer_index < self.num_coupling + self.num_final_coupling:\n",
    "            binary_masks = partition_mask(shape)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid coupling layer index.\")\n",
    "        return binary_masks if layer_index % 2 == 0 else binary_masks[::-1]\n",
    "\n",
    "    def _apply_transformation(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        layer_index: int,\n",
    "        mask: torch.Tensor,\n",
    "        complement_mask: torch.Tensor,\n",
    "        is_reverse: bool = False\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Compute scaling and translation functions for each coupling layer\n",
    "        t = self.t_scale[layer_index] * self.t[layer_index](mask * x) + self.t_bias[layer_index]\n",
    "        s = self.s_scale[layer_index] * torch.tanh(self.s[layer_index](mask * x))\n",
    "        # Apply transformation\n",
    "        if is_reverse:\n",
    "            y = x\n",
    "            output = mask * y + complement_mask * ((y - t) * torch.exp(-s))\n",
    "        else:\n",
    "            output = mask * x + complement_mask * (x * torch.exp(s) + t)\n",
    "        return output, s\n",
    "\n",
    "    def __call__(self, x: torch.Tensor) -> Output | torch.Tensor:\n",
    "        return super().__call__(x)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Forward pass through the normalizing flow model\n",
    "        if self.training:\n",
    "            # List to collect scaling outputs / batch normalizaiton layers / outputs from each coupling layer\n",
    "            norm_vals = []\n",
    "            s_vals = []\n",
    "            y_vals = []\n",
    "\n",
    "            # Process through each coupling layer\n",
    "            for i in range(self.num_coupling):\n",
    "                shape = self.shapes[i]\n",
    "                mask, complement_mask = self._get_binary_masks(shape, i)\n",
    "                y, s = self._apply_transformation(x, i, mask, complement_mask)\n",
    "                s_vals.append(torch.flatten(complement_mask * s))\n",
    "\n",
    "                # Apply batch normalization if available and collect outputs\n",
    "                y, norm_loss = self.norms[i](y)\n",
    "                norm_vals.append(norm_loss)\n",
    "\n",
    "                # Update shape for pixel operations\n",
    "                if i % 6 == 2:\n",
    "                    y = torch.nn.functional.pixel_unshuffle(y, 2)\n",
    "\n",
    "                # Manage channel factors for dimension management\n",
    "                if i % 6 == 5:\n",
    "                    factor_channels = y.shape[1] // 2\n",
    "                    y_vals.append(torch.flatten(y[:, factor_channels:, :, :], 1))\n",
    "                    y = y[:, :factor_channels, :, :]\n",
    "\n",
    "                x = y\n",
    "\n",
    "            # Apply final coupling layers\n",
    "            for i in range(self.num_coupling, self.num_coupling + self.num_final_coupling):\n",
    "                shape = self.shapes[i]\n",
    "                mask, complement_mask = self._get_binary_masks(shape, i)\n",
    "                y, s = self._apply_transformation(x, i, mask, complement_mask)\n",
    "                s_vals.append(torch.flatten(complement_mask * s))\n",
    "\n",
    "                y, norm_loss = self.norms[i](y)\n",
    "                norm_vals.append(norm_loss)\n",
    "                x = y\n",
    "\n",
    "            y_vals.append(torch.flatten(y, 1))\n",
    "\n",
    "            # Aggregate outputs and various losses for determinant computation\n",
    "            return (\n",
    "                torch.flatten(torch.cat(y_vals, 1), 1),\n",
    "                torch.cat(s_vals),\n",
    "                torch.cat([torch.flatten(v) for v in norm_vals]) if len(norm_vals) > 0 else torch.zeros(1),\n",
    "                torch.cat([torch.flatten(s) for s in self.s_scale])\n",
    "            )\n",
    "        else:\n",
    "            # Reverse transformation for data generation\n",
    "            y = x\n",
    "            layer_vars = np.prod(self.shapes[-1])\n",
    "            y_remaining = y[:, :-layer_vars]\n",
    "            y = torch.reshape(y[:, -layer_vars:], (-1,) + self.shapes[-1])\n",
    "\n",
    "            # Reversed operations for final checkerboard and coupling layers\n",
    "            for i in reversed(range(self.num_coupling, self.num_coupling + self.num_final_coupling)):\n",
    "                y, _ = self.norms[i](y)\n",
    "                shape = self.shapes[i]\n",
    "                masks = self._get_binary_masks(shape, i)\n",
    "\n",
    "                x, _ = self._apply_transformation(y, i, *masks, is_reverse=True)\n",
    "                y = x\n",
    "\n",
    "            # Prepate for multi-scale operations\n",
    "            layer_vars = np.prod(shape)\n",
    "            y = torch.cat((y, torch.reshape(y_remaining[:, -layer_vars:], (-1,) + shape)), 1)\n",
    "            y_remaining = y_remaining[:, :-layer_vars]\n",
    "\n",
    "            # Multi-scale coupling layers (Reverse transformations for earlier layers)\n",
    "            for i in reversed(range(self.num_coupling)):\n",
    "                shape = self.shapes[i]\n",
    "                masks = self._get_binary_masks(shape, i)\n",
    "\n",
    "                y, _ = self.norms[i](y)\n",
    "                x, _ = self._apply_transformation(y, i, *masks, is_reverse=True)\n",
    "\n",
    "                if i % 6 == 3:\n",
    "                    x = torch.nn.functional.pixel_shuffle(x, 2)\n",
    "\n",
    "                y = x\n",
    "\n",
    "                if i > 0 and i % 6 == 0:\n",
    "                    layer_vars = np.prod(shape)\n",
    "                    y = torch.cat((y, torch.reshape(y_remaining[:, -layer_vars:], (-1,) + shape)), 1)\n",
    "                    y_remaining = y_remaining[:, :-layer_vars]\n",
    "\n",
    "            assert np.prod(y_remaining.shape) == 0\n",
    "            return x\n",
    "\n",
    "    def _compute_loss(\n",
    "        self, y: torch.Tensor, s: torch.Tensor, norms: torch.Tensor, scale: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # loss = priori gaussiana + determinante + batch_norm_scalers\n",
    "        # loss = - (-0.5 * log(2 pi) - 0.5 * y**2 + s1 + s2 + ... + batch_norm_scalers)\n",
    "        # Priori gaussiana\n",
    "        # log_prior = -torch.sum(0.5 * (torch.log(self.TWO_PI) + y**2))\n",
    "        log_prior = -torch.sum(0.5 * torch.log(self.TWO_PI) + 0.5 * y**2)\n",
    "        log_determinant = torch.sum(s)\n",
    "        batch_norms = torch.sum(norms)\n",
    "        l2_penalty = self.l2_regularization * torch.sum(scale**2)\n",
    "        loss = -(log_prior + log_determinant + batch_norms) + l2_penalty\n",
    "        return loss / y.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_batch_count(data: DataLoader) -> int:\n",
    "        return int(\n",
    "            (np.floor if data.drop_last else np.ceil)(len(data.dataset) / data.batch_size)\n",
    "        )\n",
    "\n",
    "    def _handle_interrupt(self, sig: int, frame: FrameType) -> None:\n",
    "        # A primeira interrupção espera o fim da iteração, mas a segunda é imediata\n",
    "        if self.should_stop:\n",
    "            signal.default_int_handler(sig, frame)\n",
    "        self.should_stop = True\n",
    "\n",
    "    def get_best(self, validation: bool = True) -> tuple[int, float]:\n",
    "        \"\"\"Obtém as melhores época e loss do modelo.\"\"\"\n",
    "        history = self.validation_loss_history if validation else self.train_loss_history\n",
    "        try:\n",
    "            index = np.argmin(history)\n",
    "            return index + 1, history[index]\n",
    "        except ValueError:\n",
    "            return 0, torch.nan\n",
    "\n",
    "    def fit(\n",
    "        self, train_data: DataLoader, val_data: DataLoader, epochs: int, verbose: int = 2\n",
    "    ) -> Self:\n",
    "        epoch_loss = 0.0\n",
    "        total_batches = self._get_batch_count(train_data)\n",
    "        self.save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        epoch_iterable = tqdm(\n",
    "            range(self.epoch, epochs + 1), \"Training\", disable=verbose < 1, unit=\"epoch\"\n",
    "        )\n",
    "        batch_iterable = tqdm(\n",
    "            train_data, total=total_batches, disable=verbose < 2, unit=\" batch\"\n",
    "        )\n",
    "        self.should_stop = False\n",
    "        signal.signal(signal.SIGINT, self._handle_interrupt)\n",
    "        try:\n",
    "            for epoch in epoch_iterable:\n",
    "                if self.should_stop:\n",
    "                    break\n",
    "                self.train()\n",
    "                epoch_loss = 0.0\n",
    "                if verbose >= 2:\n",
    "                    batch_iterable.disable = False\n",
    "                    batch_iterable.reset()\n",
    "\n",
    "                for x, _ in batch_iterable:\n",
    "                    self.optimizer.zero_grad()\n",
    "                    y, s, norm_loss, s_scale = self(x)\n",
    "                    loss = self._compute_loss(y, s, norm_loss, s_scale)\n",
    "                    loss.backward()\n",
    "                    if self.gradient_clip_norm:\n",
    "                        clip_grad_norm_(self.parameters(), self.gradient_clip_norm)\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    # loss -= torch.sum(log_preprocessing_grad(x)) / x.shape[0]\n",
    "                    batch_loss = loss.item()\n",
    "                    epoch_loss += batch_loss\n",
    "                    batch_iterable.set_postfix(loss=batch_loss, refresh=False)\n",
    "\n",
    "                self.scheduler.step()\n",
    "                train_loss = epoch_loss / total_batches\n",
    "                self.train_loss_history.append(train_loss)\n",
    "\n",
    "                val_loss = self.evaluate(val_data)\n",
    "                self.validation_loss_history.append(val_loss)\n",
    "\n",
    "                best_epoch, best_loss = self.get_best()\n",
    "                epoch_iterable.set_postfix(\n",
    "                    loss=train_loss,\n",
    "                    val_loss=val_loss,\n",
    "                    best_loss=best_loss,\n",
    "                    best_epoch=best_epoch,\n",
    "                    refresh=False\n",
    "                )\n",
    "\n",
    "                if best_epoch == epoch:\n",
    "                    torch.save(self.state_dict(), self.save_path)\n",
    "                self.epoch += 1\n",
    "\n",
    "                if check_parameters(self):\n",
    "                    raise RuntimeError(\n",
    "                        f\"Invalid model parameters at the end of epoch {self.epoch - 1}.\"\n",
    "                    )\n",
    "        except:\n",
    "            signal.signal(signal.SIGINT, signal.default_int_handler)\n",
    "            raise\n",
    "\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(\n",
    "        self, data: DataLoader, get_train_output: bool = False\n",
    "    ) -> Generator[torch.Tensor | Output, None, None]:\n",
    "        self.train(get_train_output)\n",
    "        for x, _ in data:\n",
    "            yield self(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, data: DataLoader | Iterable[Output], verbose: int = 0) -> float:\n",
    "        total_loss = 0.0\n",
    "        iterable = tqdm(\n",
    "            self.predict(data, True) if isinstance(data, DataLoader) else data,\n",
    "            disable=verbose < 1, unit=\" batch\"\n",
    "        )\n",
    "        for i, (y, s, norms, scale) in enumerate(iterable, start=1):\n",
    "            loss = self._compute_loss(y, s, norms, scale)\n",
    "            pred_loss = loss.item()\n",
    "            total_loss += pred_loss\n",
    "            iterable.set_postfix(loss=pred_loss, refresh=False)\n",
    "        return total_loss / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 50\n",
    "epochs = 1\n",
    "seed = 20\n",
    "\n",
    "model = NormalizingFlowNVP(norm_type=\"batch\", gradient_clip_norm=2, seed=seed, num_coupling=12, num_final_coupling=4, planes=64)\n",
    "train_loader = DataLoader(train_dataset_cifar, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset_cifar, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81440ad8f094b6f835a5a4add7840eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39174e9587dc45efa7a4c0495420a378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ? batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(train_loader, test_loader, epochs)\n",
    "model.load_state_dict(torch.load(model.save_path))\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Best Model Path: models/best_model.pth\n",
      "Best Loss: 4714.68\n",
      "Best Epoch: 1\n"
     ]
    }
   ],
   "source": [
    "best_epoch, best_loss = model.get_best()\n",
    "print(\n",
    "    \"Done!\",\n",
    "    f\"Best Model Path: {model.save_path}\",\n",
    "    f\"Best Loss: {best_loss:.2f}\",\n",
    "    f\"Best Epoch: {best_epoch}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4714.596796875"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset_cifar, batch_size=100)\n",
    "model.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"models/history.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"loss\": model.train_loss_history,\n",
    "        \"val_loss\": model.validation_loss_history\n",
    "    }, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
