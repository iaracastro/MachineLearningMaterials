{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset_cifar = datasets.CIFAR10('data', train=True, download=True,\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "test_dataset_cifar = datasets.CIFAR10('data', train=False, download=True,\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://github.com/bjlkeng/sandbox/blob/master/realnvp/pytorch-realnvp-cifar10.ipynb\n",
    "check_mask = {}\n",
    "check_mask_device = {}\n",
    "# Partitions the input image into two sets of variables\n",
    "def partition_mask(shape, to_device=True):\n",
    "    global check_mask, check_mask_device\n",
    "    if shape not in check_mask:\n",
    "        check_mask[shape] = 1 - np.indices(shape).sum(axis=0) % 2 \n",
    "        check_mask[shape] = torch.Tensor(check_mask[shape])\n",
    "        \n",
    "    if to_device and shape not in check_mask_device:\n",
    "        check_mask_device[shape] = check_mask[shape].to(device)\n",
    "        \n",
    "    return check_mask_device[shape] if to_device else check_mask[shape]\n",
    "\n",
    "\n",
    "chan_mask = {}\n",
    "chan_mask_device = {}\n",
    "# Segregrates the channels into two groups, which transformations are independently applied\n",
    "def channel_mask(shape, to_device=True):\n",
    "    global chan_mask, chan_mask_device\n",
    "    if shape not in chan_mask:\n",
    "        chan_mask[shape] = torch.cat([torch.zeros((shape[0] // 2, shape[1], shape[2])),\n",
    "                                      torch.ones((shape[0] // 2, shape[1], shape[2])),],\n",
    "                                      dim=0)\n",
    "        assert chan_mask[shape].shape == shape, (chan_mask[shape].shape, shape)\n",
    "        \n",
    "    if to_device and shape not in chan_mask_device:\n",
    "        chan_mask_device[shape] = chan_mask[shape].to(device)\n",
    "        \n",
    "    return chan_mask_device[shape] if to_device else chan_mask[shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for Normalizing Flows with Real NVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=False))\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = weight_norm(nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=False))\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += x\n",
    "        return self.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizingFlow_NVP(nn.Module):\n",
    "    def __init__(self, num_coupling=18, num_final_coupling=4, planes=64, shape=(3,32,32)):\n",
    "        super(NormalizingFlow_NVP, self).__init__()\n",
    "        self.num_coupling = num_coupling # Number of initial coupling layers\n",
    "        self.num_final_coupling = num_final_coupling # Number of final coupling layers\n",
    "        self.shape = shape # Shape of the input image\n",
    "        \n",
    "        self.planes = planes # Number of output planes in the convolutional layers\n",
    "        self.s = nn.ModuleList() # Scaling functions for each coupling layer\n",
    "        self.t = nn.ModuleList() # Translation functions for each coupling layer \n",
    "        self.norms = nn.ModuleList() # List of batch normalization layers \n",
    "        \n",
    "        # Learnable scalar scaling parameters for outputs of s and t\n",
    "        self.s_scale = nn.ParameterList()\n",
    "        self.t_scale = nn.ParameterList()\n",
    "        self.t_bias = nn.ParameterList()\n",
    "        self.shapes = []\n",
    "\n",
    "        # Change shape and planes to increase model's capacity\n",
    "        for i in range(num_coupling):\n",
    "            self.append_transformations(shape, planes)\n",
    "            if i % 6 == 2:\n",
    "                shape = (4 * shape[0], shape[1] // 2, shape[2] // 2)\n",
    "            if i % 6 == 5:\n",
    "                # Factoring out half the channels\n",
    "                shape = (shape[0] // 2, shape[1], shape[2])\n",
    "                planes = 2 * planes\n",
    "       \n",
    "        # Setup final coupling layers with possibly different configurations\n",
    "        for i in range(num_final_coupling):\n",
    "            self.append_transformations(shape, planes)\n",
    "           \n",
    "        self.validation = False # Flag to indicate if the model is in validation mode\n",
    "    \n",
    "    def conv_stack(self, shape:int, planes: int):\n",
    "        # A common stack of convolutional blocks used in s and t functions\n",
    "        return nn.Sequential(\n",
    "            weight_norm(nn.Conv2d(shape[0], planes, kernel_size=3, stride=1, padding=1, groups=1, bias=False, dilation=1)),\n",
    "            nn.InstanceNorm2d(planes),\n",
    "            ConvBlock(planes, planes),\n",
    "            ConvBlock(planes, planes),\n",
    "            ConvBlock(planes, planes),\n",
    "            ConvBlock(planes, planes),\n",
    "            weight_norm(nn.Conv2d(planes, shape[0], kernel_size=3, stride=1, padding=1, groups=1, bias=False, dilation=1)),\n",
    "            nn.InstanceNorm2d(planes),\n",
    "        )\n",
    "    \n",
    "    def append_transformations(self, shape: int, planes: int):\n",
    "        # Append scaling and translation functions for each coupling layer\n",
    "        self.s.append(self.conv_stack(shape, planes))\n",
    "        self.t.append(self.conv_stack(shape, planes))\n",
    "        self.s_scale.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "        self.t_scale.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "        self.t_bias.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True)) \n",
    "        self.norms.append(nn.BatchNorm2d(shape[0]))\n",
    "        self.shapes.append(shape)\n",
    "\n",
    "    def validate(self): \n",
    "        # Set the model to validation mode\n",
    "        self.eval()\n",
    "        self.validation = True\n",
    "\n",
    "    def train(self, mode=True): \n",
    "        # Set the model to training mode\n",
    "        nn.Module.train(self, mode)\n",
    "        self.validation = False\n",
    "\n",
    "    def get_binary_mask(self, shape: int, i: int):\n",
    "        # Apply mask to manage which parts of the data are transformed\n",
    "        if i in range(self.num_coupling):\n",
    "            binary_mask = partition_mask(shape) if i % 6 < 3 else channel_mask(shape)\n",
    "        elif i in range(self.num_coupling, self.num_coupling + self.num_final_coupling):\n",
    "            binary_mask = partition_mask(shape)  \n",
    "        else:\n",
    "            raise ValueError(\"Invalid coupling layer index.\")\n",
    "        return  binary_mask if i % 2 == 0 else (1 - binary_mask)\n",
    "\n",
    "\n",
    "    def set_transformation(self, is_reverse, b: torch.Tensor, input_tensor: torch.Tensor, layer_i: int):\n",
    "        # Compute scaling and translation functions for each coupling layer\n",
    "        t = (self.t_scale[layer_i]) * self.t[layer_i](b * input_tensor) + (self.t_bias[layer_i])\n",
    "        s = (self.s_scale[layer_i]) * torch.tanh(self.s[layer_i](b * input_tensor))\n",
    "        # Apply transformation\n",
    "        if not is_reverse:\n",
    "            x = input_tensor \n",
    "            return b * x + (1 - b) * (x * torch.exp(s) + t), s\n",
    "        else:\n",
    "            y = input_tensor \n",
    "            return b * y + (1 - b) * ((y - t) * torch.exp(-s))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Forward pass through the normalizing flow model\n",
    "        if self.training or self.validation:\n",
    "            # List to collect scaling outputs  / batch normalizaiton layers / outputs from each coupling layer\n",
    "            s_vals = [] \n",
    "            norm_vals = [] \n",
    "            y_vals = [] \n",
    "\n",
    "            # Process through each coupling layer\n",
    "            for i in range(self.num_coupling):\n",
    "                shape = self.shapes[i]\n",
    "                b_mask = self.get_binary_mask(shape, i)\n",
    "                print(i)\n",
    "                print(\"b_mask\", b_mask.shape)\n",
    "                print(\"x\", x.shape)\n",
    "                print(\"self.s_scale[i]\", self.s_scale[i].shape)\n",
    "                print(\"self.t_scale[i]\", self.t_scale[i].shape)\n",
    "                print(\"bmask * x\", (b_mask * x).shape)\n",
    "                print(\"self.t_bias[i]\", self.t_bias[i].shape)\n",
    "                print(\"self.s[i](b_mask*x)\", self.s[i](b_mask * x).shape)\n",
    "                print(\"self.t[i](b_mask*x)\", self.t[i](b_mask * x).shape)\n",
    "                y, s = self.set_transformation(False, b_mask, x, i)\n",
    "                s_vals.append(torch.flatten((1 - b_mask) * s))\n",
    "\n",
    "                # Apply batch normalization if available and collect outputs\n",
    "                if self.norms[i] is not None:\n",
    "                    y, norm_loss = self.norms[i](y)\n",
    "                    norm_vals.append(norm_loss)\n",
    "\n",
    "                # Update shape for pixel operations\n",
    "                if i % 6 == 2:\n",
    "                    y = torch.nn.functional.pixel_unshuffle(y, 2)\n",
    "\n",
    "                # Manage channel factors for dimension management\n",
    "                if i % 6 == 5:\n",
    "                    factor_channels = y.shape[1] // 2\n",
    "                    y_vals.append(torch.flatten(y[:, factor_channels:, :, :], 1))\n",
    "                    y = y[:, :factor_channels, :, :]\n",
    "\n",
    "                x = y\n",
    "\n",
    "            # Apply final coupling layers\n",
    "            for i in range(self.num_coupling, self.num_coupling + self.num_final_coupling):\n",
    "                shape = self.shapes[i]\n",
    "                b_mask = self.get_binary_mask(shape, i)\n",
    "                y, s = self.set_transformation(False, b_mask, x, i)\n",
    "                s_vals.append(torch.flatten((1 - b_mask) * s))\n",
    "\n",
    "                if self.norms[i] is not None:\n",
    "                    y, norm_loss = self.norms[i](y)\n",
    "                    norm_vals.append(norm_loss)\n",
    "\n",
    "                x = y\n",
    "\n",
    "            y_vals.append(torch.flatten(y, 1))\n",
    "\n",
    "            # Aggregate outputs and various losses for determinant computation\n",
    "            return (torch.flatten(torch.cat(y_vals, 1), 1),\n",
    "                    torch.cat(s_vals), \n",
    "                    torch.cat([torch.flatten(v) for v in norm_vals]) if len(norm_vals) > 0 else torch.zeros(1),\n",
    "                    torch.cat([torch.flatten(s) for s in self.s_scale]))\n",
    "        else:\n",
    "            # Reverse transformation for data generation\n",
    "            y = x\n",
    "            y_remaining = y\n",
    "\n",
    "            layer_vars = np.prod(self.shapes[-1])\n",
    "            y = torch.reshape(y_remaining[:, -layer_vars:], (-1,) + self.shapes[-1])\n",
    "            y_remaining = y_remaining[:, :-layer_vars]\n",
    "\n",
    "            # Reversed operations for final checkerboard and coupling layers\n",
    "            for i in reversed(range(self.num_coupling, self.num_coupling + self.num_final_coupling)):\n",
    "                if self.norms[i] is not None:\n",
    "                    y, _ = self.norms[i](y)\n",
    "              \n",
    "                shape = self.shapes[i]\n",
    "                b_mask = self.get_binary_mask(shape, i)\n",
    "                x = self.set_transformation(True, b_mask, y, i)\n",
    "                y = x           \n",
    "\n",
    "            # Prepate for multi-scale operations\n",
    "            layer_vars = np.prod(shape)\n",
    "            y = torch.cat((y, torch.reshape(y_remaining[:, -layer_vars:], (-1,) + shape)), 1)\n",
    "            y_remaining = y_remaining[:, :-layer_vars]\n",
    "\n",
    "            # Multi-scale coupling layers (Reverse transformations for earlier layers)\n",
    "            for i in reversed(range(self.num_coupling)):\n",
    "                shape = self.shapes[i]\n",
    "                b_mask = self.get_binary_mask(shape, i)\n",
    "\n",
    "                if self.norms[i] is not None:\n",
    "                    y, _ = self.norms[i](y)\n",
    "\n",
    "                x = self.set_transformation(True, b_mask, y, i)\n",
    "\n",
    "                if i % 6 == 3:\n",
    "                    x = torch.nn.functional.pixel_shuffle(x, 2)\n",
    "\n",
    "                y = x\n",
    "\n",
    "                if i > 0 and i % 6 == 0:\n",
    "                    layer_vars = np.prod(shape)\n",
    "                    y = torch.cat((y, torch.reshape(y_remaining[:, -layer_vars:], (-1,) + shape)), 1)\n",
    "                    y_remaining = y_remaining[:, :-layer_vars]\n",
    "\n",
    "            assert np.prod(y_remaining.shape) == 0\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(y, s, norms, batch_size, scale):\n",
    "    # loss = priori gaussiana + determinante + batch_norm_scalers\n",
    "    # loss = - (-0.5 * log(2 pi) - 0.5 * y**2 + s1 + s2 + ... + batch_norm_scalers)\n",
    "    log_prior = -torch.sum(0.5 * torch.log(2 * torch.tensor(np.pi)) + 0.5 * y ** 2) # priori gaussiana\n",
    "    log_determinant = torch.sum(s) \n",
    "    batch_norms = torch.sum(norms)\n",
    "    l2_penalty = 0.0001 * torch.sum(scale ** 2)\n",
    "    loss = -(log_prior + log_determinant + batch_norms) + l2_penalty\n",
    "    return torch.div(loss, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = 32*32*3\n",
    "def pre_processing(x):\n",
    "    x = x * 255. # 0-1 -> 0-255\n",
    "    x += torch.rand_like(x) # Add random uniform [0, 1] noise \n",
    "    x = torch.logit(0.05 + 0.95 * x / 256) # Apply transform to deal with boundary effects\n",
    "    return x\n",
    "\n",
    "def train_model(model, optimizer, batch_size, report_iters=10):\n",
    "    train_loader = DataLoader(train_dataset_cifar, batch_size=batch_size, shuffle=True)\n",
    "    avg_loss, size = 0, len(train_loader.dataset)\n",
    "    for batch, (X, _) in enumerate(train_loader):\n",
    "        X = pre_processing(X)\n",
    "        X = X.to(device)\n",
    "\n",
    "        # Compute loss and gradients\n",
    "        y, s, norms, scale = model(X)\n",
    "        loss = get_loss(y, s, norms, batch_size, scale)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Report progress\n",
    "        loss += loss.item()\n",
    "        if (batch + 1) % report_iters == 0:\n",
    "            print(f'Batch {batch + 1}/{size // batch_size}; Loss: {loss}')\n",
    "\n",
    "    avg_loss = loss / (batch + 1)\n",
    "    #print(f\"Avg. Loss: {avg_loss:.2f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def test_model(model, batch_size):\n",
    "    test_loader = DataLoader(test_dataset_cifar, batch_size, shuffle=True)\n",
    "    test_loss, size = 0, len(test_loader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.validate()\n",
    "        for X, _ in test_loader:\n",
    "            X = pre_processing(X)\n",
    "            X = X.to(device)\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            y, s, norms, scale = model(X)\n",
    "            loss = get_loss(y, s, norms, batch_size, scale)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            #test_loss -= torch.sum(log_preprocessing_gradient(X)) / batch_size\n",
    "        model.train()\n",
    "    \n",
    "    test_loss /= size\n",
    "    #print(f\"Test Error: \\n Avg. Loss: {test_loss:.2f}\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model = NormalizingFlow_NVP().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "PATH = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "b_mask torch.Size([3, 32, 32])\n",
      "x torch.Size([20, 3, 32, 32])\n",
      "self.s_scale[i] torch.Size([3, 32, 32])\n",
      "self.t_scale[i] torch.Size([3, 32, 32])\n",
      "bmask * x torch.Size([20, 3, 32, 32])\n",
      "self.t_bias[i] torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iaram\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.s[i](b_mask*x) torch.Size([20, 3, 32, 32])\n",
      "self.t[i](b_mask*x) torch.Size([20, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[201], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m best_val_loss, best_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m test_model(model, batch_size)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[1;32mIn[199], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, batch_size, report_iters)\u001b[0m\n\u001b[0;32m     13\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Compute loss and gradients\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m y, s, norms, scale \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m get_loss(y, s, norms, batch_size, scale)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[196], line 117\u001b[0m, in \u001b[0;36mNormalizingFlow_NVP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Apply batch normalization if available and collect outputs\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     y, norm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms[i](y)\n\u001b[0;32m    118\u001b[0m     norm_vals\u001b[38;5;241m.\u001b[39mappend(norm_loss)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Update shape for pixel operations\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Training\n",
    "best_val_loss, best_path = float('inf'), None\n",
    "for t in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    train_loss = train_model(model, optimizer, batch_size)\n",
    "    val_loss = test_model(model, batch_size)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss, best_path = val_loss, PATH + f'cifar10_model_{t + 1}.pth'\n",
    "        torch.save(model.state_dict(), os.path.join(best_path))\n",
    "    scheduler.step()\n",
    "\n",
    "print(f\"Done! \\n Best Path: {PATH + f'cifar10_model_{t + 1}.pth'} \\n Best Loss: {best_val_loss:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
